# AI Detection Methodology

Comprehensive documentation of the AI assistance detection system used in the North Star Metrics framework to identify AI-assisted engineering work.

## Table of Contents

1. [Overview](#overview)
2. [Detection Methods](#detection-methods)
3. [Pattern Recognition](#pattern-recognition)
4. [Developer Override System](#developer-override-system)
5. [Tool Attribution](#tool-attribution)
6. [Accuracy and Validation](#accuracy-and-validation)
7. [Configuration](#configuration)
8. [Troubleshooting](#troubleshooting)

## Overview

### Purpose

The AI detection methodology aims to accurately identify engineering work that was created or significantly assisted by AI tools, enabling organizations to:

- Measure AI adoption and ROI
- Understand productivity impacts
- Track tool effectiveness
- Make informed decisions about AI investments
- Compare AI-assisted vs manual work quality

### Detection Philosophy

The system uses a **multi-layered approach** combining:

1. **Pattern-based detection** - Automatic recognition of AI tool signatures
2. **Developer overrides** - Manual attribution for known AI users
3. **Heuristic analysis** - Behavioral pattern recognition
4. **Validation feedback** - Continuous improvement through manual verification

### Accuracy Goals

- **Primary Goal**: 80%+ accuracy in AI detection
- **False Positive Rate**: <10%
- **False Negative Rate**: <20%
- **Manual Override Coverage**: 90%+ of known AI users

## Detection Methods

### 1. Pattern-Based Detection

#### Commit Message Patterns

The system searches for explicit AI tool mentions in commit messages:

```python
# Primary patterns (high confidence)
ai_patterns = [
    # Co-authorship indicators
    (r"co-authored-by:\s*github\s*copilot", "GitHub Copilot"),
    (r"co-authored-by:\s*copilot", "GitHub Copilot"),
    
    # Direct tool mentions
    (r"generated with claude", "Claude"),
    (r"claude\s*(code|ai|assisted)", "Claude"),
    (r"cursor\.ai", "Cursor"),
    (r"github\s*copilot", "GitHub Copilot"),
    (r"ai\s*(assisted|generated|help)", "Unknown AI Tool"),
    
    # Specific patterns
    (r"ðŸ¤–.*claude", "Claude"),
    (r"chatgpt|gpt-[0-9]", "ChatGPT"),
    (r"anthropic|claude", "Claude"),
]
```

#### Pull Request Description Patterns

Extended pattern matching in PR bodies and descriptions:

```python
# PR body patterns
pr_patterns = [
    # Tool-specific signatures
    (r"created with cursor", "Cursor"),
    (r"pair programming with ai", "Unknown AI Tool"),
    (r"ai code review", "Unknown AI Tool"),
    (r"copilot suggestions", "GitHub Copilot"),
    
    # Behavioral indicators
    (r"rapid development", "Possible AI"),
    (r"automated refactoring", "Unknown AI Tool"),
]
```

#### Code Comment Patterns

Detection within code comments:

```python
# Comment patterns
comment_patterns = [
    # Generated code markers
    (r"// generated by", "Unknown AI Tool"),
    (r"# AI-generated", "Unknown AI Tool"),
    (r"/* cursor ai */", "Cursor"),
    (r"// copilot", "GitHub Copilot"),
]
```

### 2. Behavioral Heuristics

#### Development Velocity Indicators

```python
def analyze_velocity_patterns(commits, author):
    """Analyze development patterns that may indicate AI assistance."""
    
    # Rapid consecutive commits
    rapid_commits = detect_rapid_commit_sequences(commits)
    
    # Unusually large changes
    large_changes = detect_large_single_commits(commits)
    
    # Consistent code quality in rapid work
    quality_consistency = analyze_code_quality_consistency(commits)
    
    return {
        "rapid_development": rapid_commits > threshold,
        "large_changes": large_changes > threshold,
        "quality_consistency": quality_consistency > threshold
    }
```

#### Code Pattern Analysis

```python
def analyze_code_patterns(diff_content):
    """Analyze code patterns that may indicate AI generation."""
    
    indicators = {
        # Consistent naming conventions
        "consistent_naming": check_naming_consistency(diff_content),
        
        # Complete function implementations
        "complete_functions": check_complete_implementations(diff_content),
        
        # Comprehensive error handling
        "error_handling": check_error_handling_patterns(diff_content),
        
        # Documentation completeness
        "documentation": check_documentation_completeness(diff_content)
    }
    
    return calculate_ai_likelihood(indicators)
```

### 3. File Type Considerations

Different file types have different AI detection approaches:

#### Source Code Files
- Pattern matching in comments
- Behavioral analysis of changes
- Code quality indicators

#### Documentation Files
- Writing style analysis
- Completeness indicators
- Structure patterns

#### Configuration Files
- Generation patterns
- Comprehensive coverage
- Best practice adherence

## Pattern Recognition

### High-Confidence Patterns

These patterns provide strong evidence of AI assistance:

```python
HIGH_CONFIDENCE_PATTERNS = [
    # Explicit attribution
    r"co-authored-by:\s*github\s*copilot",
    r"generated with claude code",
    r"ðŸ¤–.*generated with.*claude",
    
    # Tool signatures
    r"cursor\.ai",
    r"anthropic.*claude",
    r"openai.*gpt",
    
    # AI-specific workflows
    r"ai pair programming",
    r"ai code review",
    r"ai-assisted development"
]
```

### Medium-Confidence Patterns

These patterns suggest possible AI assistance but require additional validation:

```python
MEDIUM_CONFIDENCE_PATTERNS = [
    # General AI mentions
    r"ai\s*(assisted|help|tool)",
    r"automated\s*(refactoring|generation)",
    r"rapid\s*development",
    
    # Behavioral indicators
    r"comprehensive\s*(test|documentation)",
    r"complete\s*implementation",
    r"extensive\s*error\s*handling"
]
```

### Low-Confidence Patterns

These patterns are tracked but don't automatically trigger AI detection:

```python
LOW_CONFIDENCE_PATTERNS = [
    # Ambiguous terms
    r"automatic",
    r"generated",
    r"assisted",
    
    # Quality indicators that might suggest AI
    r"clean\s*code",
    r"best\s*practices",
    r"comprehensive"
]
```

### Pattern Weighting System

```python
def calculate_ai_confidence(patterns_found):
    """Calculate confidence score based on detected patterns."""
    
    confidence_score = 0
    
    for pattern, tool in patterns_found:
        if pattern in HIGH_CONFIDENCE_PATTERNS:
            confidence_score += 0.8
        elif pattern in MEDIUM_CONFIDENCE_PATTERNS:
            confidence_score += 0.4
        elif pattern in LOW_CONFIDENCE_PATTERNS:
            confidence_score += 0.1
    
    # Normalize to 0-1 range
    return min(confidence_score, 1.0)
```

## Developer Override System

### Configuration Format

The developer override system is configured in `config/ai_developers.json`:

```json
{
  "always_ai_developers": [
    {
      "username": "developer_github_username",
      "email": "developer@company.com",
      "ai_tool": "Primary AI tool used",
      "percentage": 100,
      "start_date": "2024-01-01",
      "notes": "Known heavy AI user"
    }
  ]
}
```

### Override Logic

```python
def apply_developer_overrides(commit_data, ai_developers_config):
    """Apply developer-specific AI attribution overrides."""
    
    author_email = commit_data.get('author_email', '').lower()
    author_username = commit_data.get('author_username', '').lower()
    
    for dev_config in ai_developers_config['always_ai_developers']:
        # Match by email or username
        if (author_email == dev_config['email'].lower() or 
            author_username == dev_config['username'].lower()):
            
            # Check date constraints if specified
            if 'start_date' in dev_config:
                if commit_data['date'] >= dev_config['start_date']:
                    return {
                        'ai_assisted': True,
                        'ai_tool_type': dev_config['ai_tool'],
                        'confidence': 1.0,
                        'detection_method': 'developer_override'
                    }
    
    return None
```

### Override Types

#### 1. Always AI (100%)
For developers who exclusively or almost exclusively use AI tools:

```json
{
  "username": "ai_developer",
  "email": "dev@company.com",
  "ai_tool": "Claude/Cursor",
  "percentage": 100
}
```

#### 2. High AI Usage (80-95%)
For developers who frequently use AI but occasionally work manually:

```json
{
  "username": "frequent_ai_user",
  "email": "dev@company.com",
  "ai_tool": "GitHub Copilot",
  "percentage": 85
}
```

#### 3. Experimental AI (Variable)
For developers experimenting with AI tools:

```json
{
  "username": "ai_experimenter",
  "email": "dev@company.com",
  "ai_tool": "Multiple Tools",
  "percentage": 60,
  "start_date": "2024-06-01",
  "notes": "Started AI experimentation in June"
}
```

### Override Priority

The system applies overrides in this priority order:

1. **Exact Email Match** (highest priority)
2. **GitHub Username Match**
3. **Pattern-Based Detection**
4. **Heuristic Analysis**
5. **No AI Detected** (default)

## Tool Attribution

### Supported AI Tools

The system recognizes and attributes specific AI tools:

#### Code Generation Tools
- **GitHub Copilot** - Microsoft's AI pair programmer
- **Cursor** - AI-powered code editor
- **Claude Code** - Anthropic's coding assistant
- **ChatGPT/GPT** - OpenAI's language models
- **Codeium** - Free AI code completion
- **Tabnine** - AI code completion platform

#### General AI Tools
- **Claude** - Anthropic's general-purpose AI
- **ChatGPT** - OpenAI's conversational AI
- **Perplexity** - AI research assistant
- **Unknown AI Tool** - Generic classification

### Tool Detection Logic

```python
def detect_ai_tool_type(patterns_found, commit_content):
    """Determine the specific AI tool used based on patterns."""
    
    tool_indicators = {
        'GitHub Copilot': [
            'copilot', 'co-authored-by.*copilot',
            'github copilot', 'copilot suggestion'
        ],
        'Claude': [
            'claude', 'anthropic', 'ðŸ¤–.*claude',
            'generated with claude'
        ],
        'Cursor': [
            'cursor.ai', 'cursor', 'created with cursor'
        ],
        'ChatGPT': [
            'chatgpt', 'gpt-', 'openai', 'chat gpt'
        ]
    }
    
    # Score each tool based on pattern matches
    tool_scores = {}
    for tool, patterns in tool_indicators.items():
        score = sum(1 for pattern in patterns 
                   if re.search(pattern, commit_content, re.IGNORECASE))
        if score > 0:
            tool_scores[tool] = score
    
    # Return the tool with the highest score
    if tool_scores:
        return max(tool_scores.items(), key=lambda x: x[1])[0]
    
    return "Unknown AI Tool"
```

### Tool Usage Statistics

The system tracks tool usage to provide insights:

```python
def calculate_tool_statistics(unified_data):
    """Calculate AI tool usage statistics."""
    
    ai_records = unified_data[unified_data['ai_assisted'] == True]
    
    tool_stats = {
        'total_ai_assisted': len(ai_records),
        'tool_distribution': ai_records['ai_tool_type'].value_counts().to_dict(),
        'ai_adoption_rate': len(ai_records) / len(unified_data),
        'tool_effectiveness': {}
    }
    
    # Calculate effectiveness metrics per tool
    for tool in ai_records['ai_tool_type'].unique():
        tool_data = ai_records[ai_records['ai_tool_type'] == tool]
        tool_stats['tool_effectiveness'][tool] = {
            'avg_impact_score': tool_data['impact_score'].mean(),
            'avg_complexity': tool_data['complexity_score'].mean(),
            'usage_count': len(tool_data)
        }
    
    return tool_stats
```

## Accuracy and Validation

### Manual Validation Process

#### 1. Sample Generation
```python
def generate_validation_sample(unified_data, sample_size=50):
    """Generate stratified sample for manual validation."""
    
    # Stratify by AI detection status and work type
    samples = []
    
    # AI-detected records
    ai_records = unified_data[unified_data['ai_assisted'] == True]
    ai_sample = ai_records.sample(n=min(sample_size//2, len(ai_records)))
    samples.append(ai_sample)
    
    # Non-AI records
    non_ai_records = unified_data[unified_data['ai_assisted'] == False]
    non_ai_sample = non_ai_records.sample(n=min(sample_size//2, len(non_ai_records)))
    samples.append(non_ai_sample)
    
    return pd.concat(samples, ignore_index=True)
```

#### 2. Validation Criteria
```csv
source_id,ai_detected,ai_manual,tool_detected,tool_manual,confidence,notes
PR-1234,true,true,Claude,Claude,high,"Clear AI attribution in commit"
PR-1235,false,true,None,Cursor,medium,"Missed cursor pattern"
PR-1236,true,false,Copilot,None,low,"False positive from automation"
```

#### 3. Accuracy Metrics
```python
def calculate_validation_metrics(validation_results):
    """Calculate AI detection accuracy metrics."""
    
    # Binary classification metrics
    true_positives = len(validation_results[
        (validation_results['ai_detected'] == True) & 
        (validation_results['ai_manual'] == True)
    ])
    
    false_positives = len(validation_results[
        (validation_results['ai_detected'] == True) & 
        (validation_results['ai_manual'] == False)
    ])
    
    false_negatives = len(validation_results[
        (validation_results['ai_detected'] == False) & 
        (validation_results['ai_manual'] == True)
    ])
    
    true_negatives = len(validation_results[
        (validation_results['ai_detected'] == False) & 
        (validation_results['ai_manual'] == False)
    ])
    
    # Calculate metrics
    precision = true_positives / (true_positives + false_positives)
    recall = true_positives / (true_positives + false_negatives)
    accuracy = (true_positives + true_negatives) / len(validation_results)
    
    return {
        'precision': precision,
        'recall': recall,
        'accuracy': accuracy,
        'f1_score': 2 * (precision * recall) / (precision + recall)
    }
```

### Validation Targets

| Metric | Target | Good | Needs Improvement |
|--------|--------|------|-------------------|
| **Overall Accuracy** | 85%+ | 80-84% | <80% |
| **AI Detection Precision** | 90%+ | 85-89% | <85% |
| **AI Detection Recall** | 80%+ | 75-79% | <75% |
| **Tool Attribution Accuracy** | 75%+ | 70-74% | <70% |

### Continuous Improvement

#### 1. Pattern Evolution
```python
def update_detection_patterns(validation_results):
    """Update detection patterns based on validation feedback."""
    
    # Identify missed AI patterns
    missed_ai = validation_results[
        (validation_results['ai_detected'] == False) & 
        (validation_results['ai_manual'] == True)
    ]
    
    # Extract new patterns from missed cases
    new_patterns = extract_patterns_from_text(
        missed_ai['commit_message'].tolist() + 
        missed_ai['pr_description'].tolist()
    )
    
    # Validate new patterns before adding
    validated_patterns = validate_new_patterns(new_patterns)
    
    return validated_patterns
```

#### 2. Threshold Tuning
```python
def optimize_detection_thresholds(validation_data):
    """Optimize detection thresholds based on validation results."""
    
    from sklearn.metrics import precision_recall_curve
    
    # Get confidence scores and manual labels
    confidence_scores = validation_data['confidence_score']
    manual_labels = validation_data['ai_manual']
    
    # Find optimal threshold
    precision, recall, thresholds = precision_recall_curve(
        manual_labels, confidence_scores
    )
    
    # Optimize for F1 score
    f1_scores = 2 * (precision * recall) / (precision + recall)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]
    
    return {
        'optimal_threshold': optimal_threshold,
        'precision': precision[optimal_idx],
        'recall': recall[optimal_idx],
        'f1_score': f1_scores[optimal_idx]
    }
```

## Configuration

### Pattern Configuration

Patterns are defined in the source code but can be extended:

```python
# src/analysis/context_preparer.py
def get_ai_detection_patterns():
    """Get current AI detection patterns."""
    return {
        'high_confidence': HIGH_CONFIDENCE_PATTERNS,
        'medium_confidence': MEDIUM_CONFIDENCE_PATTERNS,
        'low_confidence': LOW_CONFIDENCE_PATTERNS,
        'tool_specific': TOOL_SPECIFIC_PATTERNS
    }
```

### Developer Override Configuration

File: `config/ai_developers.json`

```json
{
  "always_ai_developers": [
    {
      "username": "chad",
      "email": "chad@company.com",
      "ai_tool": "Claude/Cursor",
      "percentage": 100,
      "start_date": "2024-01-01",
      "notes": "Primary AI user for development",
      "validation_status": "confirmed"
    }
  ],
  "pattern_overrides": {
    "ignore_patterns": [
      "automated test generation",
      "auto-generated documentation"
    ],
    "custom_patterns": [
      {
        "pattern": "pair coded with ai",
        "tool": "Unknown AI Tool",
        "confidence": 0.8
      }
    ]
  }
}
```

### Validation Configuration

File: `config/validation.json`

```json
{
  "ai_detection_validation": {
    "sample_size": 50,
    "validation_frequency": "weekly",
    "accuracy_targets": {
      "overall_accuracy": 0.85,
      "precision": 0.90,
      "recall": 0.80,
      "tool_attribution": 0.75
    },
    "manual_review_triggers": {
      "accuracy_drop": 0.05,
      "new_patterns_found": 5,
      "high_uncertainty_rate": 0.20
    }
  }
}
```

## Troubleshooting

### Common Issues

#### 1. Low AI Detection Rate (<10%)

**Symptoms:**
- Very few records marked as AI-assisted
- Known AI users not being detected

**Possible Causes:**
- Missing developer overrides
- Insufficient pattern coverage
- Overly restrictive thresholds

**Solutions:**
```bash
# Check current detection rate
python -c "
import pandas as pd
df = pd.read_csv('unified_pilot_data.csv')
ai_rate = df['ai_assisted'].mean() * 100
print(f'Current AI detection rate: {ai_rate:.1f}%')
"

# Review developer overrides
cat config/ai_developers.json

# Run validation to identify missed patterns
python scripts/validation/validate_methodology.py unified_pilot_data.csv
```

#### 2. High False Positive Rate

**Symptoms:**
- Manual validation shows many incorrectly detected AI records
- AI rate seems unrealistically high (>90%)

**Possible Causes:**
- Overly broad patterns
- Incorrect developer overrides
- Pattern conflicts

**Solutions:**
```bash
# Analyze false positives
python scripts/validation/validate_methodology.py unified_pilot_data.csv --sample-size 30

# Review and refine patterns
# Edit src/analysis/context_preparer.py

# Check for conflicting developer overrides
python -c "
import json
with open('config/ai_developers.json') as f:
    config = json.load(f)
    for dev in config['always_ai_developers']:
        if dev['percentage'] > 95:
            print(f'High percentage override: {dev}')
"
```

#### 3. Tool Attribution Errors

**Symptoms:**
- AI detected correctly but wrong tool attributed
- Generic "Unknown AI Tool" classifications

**Possible Causes:**
- Insufficient tool-specific patterns
- Pattern conflicts between tools
- Missing tool signatures

**Solutions:**
```bash
# Analyze tool distribution
python -c "
import pandas as pd
df = pd.read_csv('unified_pilot_data.csv')
ai_df = df[df['ai_assisted'] == True]
print(ai_df['ai_tool_type'].value_counts())
"

# Review tool-specific patterns
grep -n "tool_indicators" src/analysis/context_preparer.py

# Add tool-specific patterns for common tools
```

#### 4. Inconsistent Detection Across Time

**Symptoms:**
- Detection rate varies significantly between time periods
- Sudden drops or spikes in AI detection

**Possible Causes:**
- Changes in developer behavior
- Tool adoption patterns
- Data quality issues

**Solutions:**
```bash
# Analyze temporal patterns
python -c "
import pandas as pd
df = pd.read_csv('unified_pilot_data.csv')
df['date'] = pd.to_datetime(df['date'])
monthly = df.groupby(df['date'].dt.to_period('M'))['ai_assisted'].mean()
print('Monthly AI detection rates:')
print(monthly)
"

# Check for data quality issues
python scripts/validation/check_data_quality.py unified_pilot_data.csv
```

### Validation Commands

```bash
# Comprehensive AI detection validation
python scripts/validation/validate_methodology.py unified_pilot_data.csv

# Check specific patterns
grep -i "claude\|copilot\|cursor" unified_pilot_data.csv

# Analyze developer attribution
python -c "
import pandas as pd
df = pd.read_csv('unified_pilot_data.csv')
ai_by_author = df.groupby('author')['ai_assisted'].agg(['count', 'sum', 'mean'])
ai_by_author['ai_rate'] = ai_by_author['mean']
print(ai_by_author.sort_values('ai_rate', ascending=False).head(10))
"

# Test developer overrides
python -c "
from src.config.config_manager import ConfigManager
cm = ConfigManager()
config = cm.load_ai_developers()
print('Configured AI developers:')
for dev in config['always_ai_developers']:
    print(f'  {dev[\"username\"]} ({dev[\"ai_tool\"]}, {dev[\"percentage\"]}%)')
"
```

### Performance Optimization

#### Pattern Optimization
```python
# Compile regex patterns for better performance
import re

compiled_patterns = [
    (re.compile(pattern, re.IGNORECASE), tool)
    for pattern, tool in ai_patterns
]

def fast_pattern_detection(text):
    """Optimized pattern detection using compiled regex."""
    for pattern, tool in compiled_patterns:
        if pattern.search(text):
            return True, tool
    return False, None
```

#### Batch Processing
```python
def batch_ai_detection(records, batch_size=100):
    """Process AI detection in batches for better performance."""
    
    results = []
    
    for i in range(0, len(records), batch_size):
        batch = records[i:i+batch_size]
        batch_results = []
        
        for record in batch:
            ai_detected, tool = detect_ai_assistance(record)
            batch_results.append({
                'ai_assisted': ai_detected,
                'ai_tool_type': tool
            })
        
        results.extend(batch_results)
    
    return results
```

---

For more information:
- [Configuration Reference](configuration-reference.md) - AI developer override setup
- [Validation Procedures](validation-procedures.md) - Manual validation process
- [Setup Guide](setup-guide.md) - Initial configuration
- [Troubleshooting Guide](troubleshooting.md) - Common issues and solutions